diff --git a/arch/x86/include/asm/bootparam.h b/arch/x86/include/asm/bootparam.h
index e020d88..c858b30 100644
--- a/arch/x86/include/asm/bootparam.h
+++ b/arch/x86/include/asm/bootparam.h
@@ -57,7 +57,9 @@ struct setup_header {
 	__u32	initrd_addr_max;
 	__u32	kernel_alignment;
 	__u8	relocatable_kernel;
-	__u8	_pad2[3];
+	__u8	ramdisk_shift;
+	__u8    ramdisk_magic;
+	__u8	_pad2[1];
 	__u32	cmdline_size;
 	__u32	hardware_subarch;
 	__u64	hardware_subarch_data;
diff --git a/arch/x86/include/asm/smp.h b/arch/x86/include/asm/smp.h
index 73b11bc..5649868 100644
--- a/arch/x86/include/asm/smp.h
+++ b/arch/x86/include/asm/smp.h
@@ -160,6 +160,8 @@ void play_dead_common(void);
 void wbinvd_on_cpu(int cpu);
 int wbinvd_on_all_cpus(void);
 
+int mkbsp_boot_cpu(int apicid, int cpu, unsigned long kernel_start_address);
+
 void native_send_call_func_ipi(const struct cpumask *mask);
 void native_send_call_func_single_ipi(int cpu);
 
diff --git a/arch/x86/include/asm/trampoline.h b/arch/x86/include/asm/trampoline.h
index feca311..9f3d180 100644
--- a/arch/x86/include/asm/trampoline.h
+++ b/arch/x86/include/asm/trampoline.h
@@ -15,25 +15,48 @@ extern const unsigned char x86_trampoline_start [];
 extern const unsigned char x86_trampoline_end   [];
 extern unsigned char *x86_trampoline_base;
 
+extern const unsigned char x86_trampoline_bsp_start [];
+extern const unsigned char x86_trampoline_bsp_end   [];
+extern unsigned char *x86_trampoline_bsp_base;
+extern unsigned long kernel_phys_addr;
+extern unsigned long boot_params_phys_addr;
+
 extern unsigned long init_rsp;
 extern unsigned long initial_code;
 extern unsigned long initial_gs;
 
+extern unsigned long mkbsp_load_addr;
+extern unsigned long mkbsp_boot_params;
+
 extern void __init setup_trampolines(void);
+extern void __init setup_trampolines_bsp(void);
 
 extern const unsigned char trampoline_data[];
 extern const unsigned char trampoline_status[];
 
+extern const unsigned char trampoline_data_bsp[];
+extern const unsigned char trampoline_status_bsp[];
+
 #define TRAMPOLINE_SYM(x)						\
 	((void *)(x86_trampoline_base +					\
 		  ((const unsigned char *)(x) - x86_trampoline_start)))
 
+#define TRAMPOLINE_SYM_BSP(x)						\
+	((void *)(x86_trampoline_bsp_base +					\
+		  ((const unsigned char *)(x) - x86_trampoline_bsp_start)))
+
 /* Address of the SMP trampoline */
 static inline unsigned long trampoline_address(void)
 {
 	return virt_to_phys(TRAMPOLINE_SYM(trampoline_data));
 }
 
+/* Address of the SMP trampoline */
+static inline unsigned long trampoline_bsp_address(void)
+{
+	return virt_to_phys(TRAMPOLINE_SYM_BSP(trampoline_data_bsp));
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* _ASM_X86_TRAMPOLINE_H */
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 8baca3c..cbf0d3d 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -35,6 +35,7 @@ obj-y			+= pci-iommu_table.o
 obj-y			+= resource.o
 
 obj-y				+= trampoline.o trampoline_$(BITS).o
+obj-$(CONFIG_POPCORN)           += trampoline_$(BITS)_bsp.o
 obj-y				+= process.o
 obj-y				+= i387.o xsave.o
 obj-y				+= ptrace.o
diff --git a/arch/x86/kernel/aperture_64.c b/arch/x86/kernel/aperture_64.c
index 3d2661c..ed4f7da 100644
--- a/arch/x86/kernel/aperture_64.c
+++ b/arch/x86/kernel/aperture_64.c
@@ -44,7 +44,13 @@
  * code for safe.
  */
 #define GART_MIN_ADDR	(512ULL << 20)
+
+#ifdef CONFIG_POPCORN
+#define GART_MAX_ADDR	(1ULL   << 64)
+#else
 #define GART_MAX_ADDR	(1ULL   << 32)
+#endif
+
 
 int gart_iommu_aperture;
 int gart_iommu_aperture_disabled __initdata;
@@ -370,6 +376,11 @@ int __init gart_iommu_hole_init(void)
 	int fix, slot, valid_agp = 0;
 	int i, node;
 
+	/* POPCORN -- disable IOMMU aperture for now */
+#ifdef CONFIG_POPCORN
+	gart_iommu_aperture_disabled = 1;
+#endif
+
 	if (gart_iommu_aperture_disabled || !fix_aperture ||
 	    !early_pci_allowed())
 		return -ENODEV;
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index aa003b1..0debb51 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1149,29 +1149,37 @@ static void dbg_restore_debug_regs(void)
  */
 #ifdef CONFIG_X86_64
 
-void __cpuinit cpu_init(void)
+void __cpuinit cpu_init( int flags)
 {
 	struct orig_ist *oist;
 	struct task_struct *me;
 	struct tss_struct *t;
 	unsigned long v;
-	int cpu;
+	int cpu, smp_cpu;
 	int i;
 
 	cpu = stack_smp_processor_id();
+	smp_cpu = smp_processor_id();
+
+	printk("%s: cpu %d smp_cpu %d virtual %p\n",__func__,  cpu, smp_cpu, &cpu);
+
+	if (flags)
+		cpu = smp_cpu;
+
 	t = &per_cpu(init_tss, cpu);
 	oist = &per_cpu(orig_ist, cpu);
 
 #ifdef CONFIG_NUMA
 	if (cpu != 0 && percpu_read(numa_node) == 0 &&
-	    early_cpu_to_node(cpu) != NUMA_NO_NODE)
+			early_cpu_to_node(cpu) != NUMA_NO_NODE)
 		set_numa_node(early_cpu_to_node(cpu));
 #endif
 
 	me = current;
 
+
 	if (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask))
-		panic("CPU#%d already initialized!\n", cpu);
+		panic("CPU#%d APICID %x unit_off=%lx already initialized!\n", cpu, read_apic_id(), per_cpu(this_cpu_off, cpu) );
 
 	pr_debug("Initializing CPU#%d\n", cpu);
 
diff --git a/arch/x86/kernel/e820.c b/arch/x86/kernel/e820.c
index 303a0e4..9487819 100644
--- a/arch/x86/kernel/e820.c
+++ b/arch/x86/kernel/e820.c
@@ -19,6 +19,7 @@
 #include <linux/acpi.h>
 #include <linux/firmware-map.h>
 #include <linux/memblock.h>
+#include <linux/multikernel.h>
 
 #include <asm/e820.h>
 #include <asm/proto.h>
@@ -824,7 +825,15 @@ unsigned long __init e820_end_of_ram_pfn(void)
 
 unsigned long __init e820_end_of_low_ram_pfn(void)
 {
+#ifdef CONFIG_POPCORN
+	if (mklinux_boot) {
+		return e820_end_pfn(1UL<<(64 - PAGE_SHIFT), E820_RAM);
+	} else {
+		return e820_end_pfn(1UL<<(32 - PAGE_SHIFT), E820_RAM);
+	}
+#else
 	return e820_end_pfn(1UL<<(32 - PAGE_SHIFT), E820_RAM);
+#endif
 }
 
 static void early_panic(char *msg)
diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index 5655c22..fbc7462 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -27,6 +27,11 @@
 #include <asm/trampoline.h>
 #include <asm/bios_ebda.h>
 
+#include <linux/module.h>
+
+unsigned long orig_boot_params;
+EXPORT_SYMBOL(orig_boot_params);
+
 static void __init zap_identity_mappings(void)
 {
 	pgd_t *pgd = pgd_offset_k(0UL);
@@ -77,7 +82,7 @@ void __init x86_64_start_kernel(char * real_mode_data)
 	/* Make NULL pointers segfault */
 	zap_identity_mappings();
 
-	max_pfn_mapped = KERNEL_IMAGE_SIZE >> PAGE_SHIFT;
+	max_pfn_mapped = (KERNEL_IMAGE_SIZE >> PAGE_SHIFT) * 256;
 
 	for (i = 0; i < NUM_EXCEPTION_VECTORS; i++) {
 #ifdef CONFIG_EARLY_PRINTK
@@ -91,6 +96,8 @@ void __init x86_64_start_kernel(char * real_mode_data)
 	if (console_loglevel == 10)
 		early_printk("Kernel alive\n");
 
+	orig_boot_params = real_mode_data;
+
 	x86_64_start_reservations(real_mode_data);
 }
 
@@ -103,12 +110,25 @@ void __init x86_64_start_reservations(char *real_mode_data)
 	memblock_x86_reserve_range(__pa_symbol(&_text), __pa_symbol(&__bss_stop), "TEXT DATA BSS");
 
 #ifdef CONFIG_BLK_DEV_INITRD
+#define RAMDISK_MAGIC 0xdf
 	/* Reserve INITRD */
 	if (boot_params.hdr.type_of_loader && boot_params.hdr.ramdisk_image) {
 		/* Assume only end is not page aligned */
-		unsigned long ramdisk_image = boot_params.hdr.ramdisk_image;
+		unsigned long ramdisk_shift = boot_params.hdr.ramdisk_shift;
+		unsigned long ramdisk_image;
 		unsigned long ramdisk_size  = boot_params.hdr.ramdisk_size;
-		unsigned long ramdisk_end   = PAGE_ALIGN(ramdisk_image + ramdisk_size);
+		unsigned long ramdisk_end;
+	
+		/* POPCORN -- the BIOS might not zero out the ramdisk_shift
+		   field, so we need to account for it */
+		if (boot_params.hdr.ramdisk_magic == RAMDISK_MAGIC) {
+			ramdisk_image = boot_params.hdr.ramdisk_image + (ramdisk_shift << 32);
+		} else {
+			ramdisk_image = boot_params.hdr.ramdisk_image;
+		}
+
+		ramdisk_end = PAGE_ALIGN(ramdisk_image + ramdisk_size);
+
 		memblock_x86_reserve_range(ramdisk_image, ramdisk_end, "RAMDISK");
 	}
 #endif
diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S
index e11e394..def11a9 100644
--- a/arch/x86/kernel/head_64.S
+++ b/arch/x86/kernel/head_64.S
@@ -103,6 +103,8 @@ startup_64:
 	andq	$(PTRS_PER_PUD - 1), %rax
 	jz	ident_complete
 
+	/* MKLINUX -- Set the appropriate entry of level3_ident_pgt to 
+	 * point to level2_spare_pgt */
 	leaq	(level2_spare_pgt - __START_KERNEL_map + _KERNPG_TABLE)(%rbp), %rdx
 	leaq	level3_ident_pgt(%rip), %rbx
 	movq	%rdx, 0(%rbx, %rax, 8)
@@ -110,9 +112,24 @@ startup_64:
 	movq	%rdi, %rax
 	shrq	$PMD_SHIFT, %rax
 	andq	$(PTRS_PER_PMD - 1), %rax
+
+	/* MKLINUX -- at this point, %rax should be 0x0
+	 * and %rdi should be rounded down to a multiple of 1 GB */
+	movq    $0, %rax
+	andq    $0xffffffffc0000000, %rdi
+
+	/* MKLINUX -- fill up level2_spare_pgt to map the 1 GB where the
+	 * kernel has been loaded */
+	movq	$512, %rcx
 	leaq	__PAGE_KERNEL_IDENT_LARGE_EXEC(%rdi), %rdx
 	leaq	level2_spare_pgt(%rip), %rbx
-	movq	%rdx, 0(%rbx, %rax, 8)
+
+1:	movq	%rdx, 0(%rbx, %rax, 8)
+	addq	$0x00200000, %rdx
+	incq	%rax
+	decq    %rcx
+        jnz     1b
+
 ident_complete:
 
 	/*
@@ -237,6 +254,7 @@ ENTRY(secondary_startup_64)
 	movl	initial_gs+4(%rip),%edx
 	wrmsr	
 
+	/* MKLINUX -- note that esi contains the pointer to the boot_params structure! */
 	/* esi is pointer to real mode structure with interesting info.
 	   pass it to C */
 	movl	%esi, %edi
diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c
index cf0ef98..843f636 100644
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@ -316,10 +316,12 @@ static void __init reserve_brk(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 
 #define MAX_MAP_CHUNK	(NR_FIX_BTMAPS << PAGE_SHIFT)
+#define RAMDISK_MAGIC 0xdf
 static void __init relocate_initrd(void)
 {
 	/* Assume only end is not page aligned */
-	u64 ramdisk_image = boot_params.hdr.ramdisk_image;
+	u64 ramdisk_shift = boot_params.hdr.ramdisk_shift;
+	u64 ramdisk_image;
 	u64 ramdisk_size  = boot_params.hdr.ramdisk_size;
 	u64 area_size     = PAGE_ALIGN(ramdisk_size);
 	u64 end_of_lowmem = max_low_pfn_mapped << PAGE_SHIFT;
@@ -327,6 +329,14 @@ static void __init relocate_initrd(void)
 	unsigned long slop, clen, mapaddr;
 	char *p, *q;
 
+	/* POPCORN -- the BIOS might not zero out the ramdisk_shift
+	   field, so we need to account for it */
+	if (boot_params.hdr.ramdisk_magic == RAMDISK_MAGIC) {
+		ramdisk_image = boot_params.hdr.ramdisk_image + (ramdisk_shift << 32);
+	} else {
+		ramdisk_image = boot_params.hdr.ramdisk_image;
+	}
+
 	/* We need to move the initrd down into lowmem */
 	ramdisk_here = memblock_find_in_range(0, end_of_lowmem, area_size,
 					 PAGE_SIZE);
@@ -381,13 +391,29 @@ static void __init relocate_initrd(void)
 static void __init reserve_initrd(void)
 {
 	/* Assume only end is not page aligned */
-	u64 ramdisk_image = boot_params.hdr.ramdisk_image;
+	u64 ramdisk_shift = boot_params.hdr.ramdisk_shift;
+	u64 ramdisk_image;
 	u64 ramdisk_size  = boot_params.hdr.ramdisk_size;
-	u64 ramdisk_end   = PAGE_ALIGN(ramdisk_image + ramdisk_size);
+	u64 ramdisk_end;
 	u64 end_of_lowmem = max_low_pfn_mapped << PAGE_SHIFT;
 
+	/* POPCORN -- the BIOS might not zero out the ramdisk_shift
+	   field, so we need to account for it */
+	if (boot_params.hdr.ramdisk_magic == RAMDISK_MAGIC) {
+		printk("ramdisk magic number is correct (0x%x), applying shift...\n", boot_params.hdr.ramdisk_magic);
+		ramdisk_image = boot_params.hdr.ramdisk_image + (ramdisk_shift << 32);
+	} else {
+		printk("ramdisk magic number is wrong (0x%x)\n", boot_params.hdr.ramdisk_magic);
+		ramdisk_image = boot_params.hdr.ramdisk_image;
+	}
+
+	ramdisk_end = PAGE_ALIGN(ramdisk_image + ramdisk_size);
+
+	printk("ramdisk_image 0x%lx, size 0x%lx, shift 0x%lx, end 0x%lx, end_of_lowmem 0x%lx\n",
+                        ramdisk_image, ramdisk_size, ramdisk_shift, ramdisk_end, end_of_lowmem);
+
 	if (!boot_params.hdr.type_of_loader ||
-	    !ramdisk_image || !ramdisk_size)
+			!ramdisk_image || !ramdisk_size)
 		return;		/* No initrd provided by bootloader */
 
 	initrd_start = 0;
@@ -395,11 +421,11 @@ static void __init reserve_initrd(void)
 	if (ramdisk_size >= (end_of_lowmem>>1)) {
 		memblock_x86_free_range(ramdisk_image, ramdisk_end);
 		printk(KERN_ERR "initrd too large to handle, "
-		       "disabling initrd\n");
+				"disabling initrd\n");
 		return;
 	}
 
-	printk(KERN_INFO "RAMDISK: %08llx - %08llx\n", ramdisk_image,
+	printk(KERN_INFO "RAMDISK: %llx - %llx\n", ramdisk_image,
 			ramdisk_end);
 
 
@@ -409,6 +435,7 @@ static void __init reserve_initrd(void)
 		 * don't need to reserve again, already reserved early
 		 * in i386_start_kernel
 		 */
+		printk("Ramdisk all in lowmem -- easy case\n");
 		initrd_start = ramdisk_image + PAGE_OFFSET;
 		initrd_end = initrd_start + ramdisk_size;
 		return;
@@ -881,12 +908,16 @@ void __init setup_arch(char **cmdline_p)
 
 	check_x2apic();
 
+	printk("max_pfn is 0x%lx\n", max_pfn);
+
 	/* How many end-of-memory variables you have, grandma! */
 	/* need this before calling reserve_initrd */
-	if (max_pfn > (1UL<<(32 - PAGE_SHIFT)))
+	if (max_pfn > (1UL<<(32 - PAGE_SHIFT))) {
+		printk("Setting max_low_pfn to e820_end_of_low_ram_pfn\n");
 		max_low_pfn = e820_end_of_low_ram_pfn();
-	else
+	} else {
 		max_low_pfn = max_pfn;
+	}
 
 	high_memory = (void *)__va(max_pfn * PAGE_SIZE - 1) + 1;
 #endif
@@ -928,9 +959,12 @@ void __init setup_arch(char **cmdline_p)
 			max_pfn_mapped<<PAGE_SHIFT);
 
 	setup_trampolines();
+	setup_trampolines_bsp();
 
 	init_gbpages();
 
+	printk("max_low_pfn 0x%lx, page_shift 0x%lx\n", max_low_pfn, PAGE_SHIFT);
+
 	/* max_pfn_mapped is updated here */
 	max_low_pfn_mapped = init_memory_mapping(0, max_low_pfn<<PAGE_SHIFT);
 	max_pfn_mapped = max_low_pfn_mapped;
@@ -1017,7 +1051,8 @@ void __init setup_arch(char **cmdline_p)
 	if (smp_found_config)
 		get_smp_config();
 
-	prefill_possible_map();
+	prefill_possible_map(); // cpu_possible_mask fill with other cpus from setup_possible_cpus, disabled_cpus...
+	prefill_present_map(); // POPCORN -- reset_present_map(); TODO HERE or before prefile_possible in order to set max_cpus from the present_map
 
 	init_cpu_to_node();
 
diff --git a/arch/x86/kernel/trampoline.c b/arch/x86/kernel/trampoline.c
index a91ae77..ddf4212 100644
--- a/arch/x86/kernel/trampoline.c
+++ b/arch/x86/kernel/trampoline.c
@@ -5,7 +5,10 @@
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
 
+extern int mklinux_boot;
+
 unsigned char *x86_trampoline_base;
+unsigned char *x86_trampoline_bsp_base;
 
 void __init setup_trampolines(void)
 {
@@ -23,7 +26,38 @@ void __init setup_trampolines(void)
 	printk(KERN_DEBUG "Base memory trampoline at [%p] %llx size %zu\n",
 	       x86_trampoline_base, (unsigned long long)mem, size);
 
-	memcpy(x86_trampoline_base, x86_trampoline_start, size);
+	/* POPCORN -- this is commented out because it breaks clustering.
+	 * TODO -- need to fix! */
+//	if (!mklinux_boot) {
+		memcpy(x86_trampoline_base, x86_trampoline_start, size);
+//	} else {
+//		printk("Popcorn boot: SMP trampoline will NOT be copied\n");
+//	}
+}
+
+
+void __init setup_trampolines_bsp(void)
+{
+	phys_addr_t mem;
+	size_t size = PAGE_ALIGN(x86_trampoline_bsp_end - x86_trampoline_bsp_start);
+
+	/* Has to be in very low memory so we can execute real-mode AP code. */
+	mem = memblock_find_in_range(0, 1<<20, size, PAGE_SIZE);
+	if (mem == MEMBLOCK_ERROR)
+		panic("Cannot allocate trampoline\n");
+
+	x86_trampoline_bsp_base = __va(mem);
+	memblock_x86_reserve_range(mem, mem + size, "TRAMPOLINE_BSP");
+
+	printk(KERN_DEBUG "Base memory trampoline BSP at [%p] %llx size %zu\n",
+	       x86_trampoline_bsp_base, (unsigned long long)mem, size);
+
+	if (!mklinux_boot) {
+		memcpy(x86_trampoline_bsp_base, x86_trampoline_bsp_start, size);
+
+	} else {
+		printk("Popcorn boot: BSP trampoline will NOT be copied\n");
+	}
 }
 
 /*
@@ -39,4 +73,16 @@ static int __init configure_trampolines(void)
 	set_memory_x((unsigned long)x86_trampoline_base, size >> PAGE_SHIFT);
 	return 0;
 }
+
 arch_initcall(configure_trampolines);
+
+static int __init configure_trampolines_bsp(void)
+{
+	size_t size = PAGE_ALIGN(x86_trampoline_bsp_end - x86_trampoline_bsp_start);
+
+	set_memory_x((unsigned long)x86_trampoline_bsp_base, size >> PAGE_SHIFT);
+	return 0;
+}
+
+arch_initcall(configure_trampolines_bsp);
+
diff --git a/arch/x86/kernel/trampoline_64_bsp.S b/arch/x86/kernel/trampoline_64_bsp.S
new file mode 100644
index 0000000..590bb7c
--- /dev/null
+++ b/arch/x86/kernel/trampoline_64_bsp.S
@@ -0,0 +1,305 @@
+/*
+ *
+ *	Trampoline.S	Derived from Setup.S by Linus Torvalds
+ *
+ *	4 Jan 1997 Michael Chastain: changed to gnu as.
+ *	15 Sept 2005 Eric Biederman: 64bit PIC support
+ *
+ *	Entry: CS:IP point to the start of our code, we are 
+ *	in real mode with no stack, but the rest of the 
+ *	trampoline page to make our stack and everything else
+ *	is a mystery.
+ *
+ *	On entry to trampoline_data, the processor is in real mode
+ *	with 16-bit addressing and 16-bit data.  CS has some value
+ *	and IP is zero.  Thus, data addresses need to be absolute
+ *	(no relocation) and are taken with regard to r_base.
+ *
+ *	With the addition of trampoline_level4_pgt this code can
+ *	now enter a 64bit kernel that lives at arbitrary 64bit
+ *	physical addresses.
+ *
+ *	If you work on this file, check the object module with objdump
+ *	--full-contents --reloc to make sure there are no relocation
+ *	entries.
+ */
+
+#include <linux/linkage.h>
+#include <linux/init.h>
+#include <asm/pgtable_types.h>
+#include <asm/page_types.h>
+#include <asm/msr.h>
+#include <asm/segment.h>
+#include <asm/processor-flags.h>
+
+	.section ".x86_trampoline_bsp","a"
+	.balign PAGE_SIZE
+	.code16
+
+ENTRY(trampoline_data_bsp)
+bsp_base = .
+	cli			# We should be safe anyway
+	wbinvd
+	mov	%cs, %ax	# Code and data in the same place
+	mov	%ax, %ds
+	mov	%ax, %es
+	mov	%ax, %ss
+
+
+	movl	$0xA5A5A5A5, trampoline_status_bsp - bsp_base
+				# write marker for master knows we're running
+
+					# Setup stack
+	movw	$(trampoline_stack_bsp_end - bsp_base), %sp
+
+	# call	verify_cpu		# Verify the cpu supports long mode
+	# testl   %eax, %eax		# Check for return code
+	# jnz	no_longmode_bsp
+
+	mov	%cs, %ax
+	movzx	%ax, %esi		# Find the 32bit trampoline location
+	shll	$4, %esi
+
+					# Fixup the absolute vectors
+	leal	(startup_32_bsp - bsp_base)(%esi), %eax
+	movl	%eax, startup_32_vector_bsp - bsp_base
+	leal	(startup_64_bsp - bsp_base)(%esi), %eax
+	movl	%eax, startup_64_vector_bsp - bsp_base
+	leal	(tgdt_bsp - bsp_base)(%esi), %eax
+	movl	%eax, (tgdt_bsp + 2 - bsp_base)
+
+	/*
+	 * GDT tables in non default location kernel can be beyond 16MB and
+	 * lgdt will not be able to load the address as in real mode default
+	 * operand size is 16bit. Use lgdtl instead to force operand size
+	 * to 32 bit.
+	 */
+
+	lidtl	tidt_bsp - bsp_base	# load idt with 0, 0
+	lgdtl	tgdt_bsp - bsp_base	# load gdt with whatever is appropriate
+
+	mov	$X86_CR0_PE, %ax	# protected mode (PE) bit
+	lmsw	%ax			# into protected mode
+
+	# flush prefetch and jump to startup_32
+	ljmpl	*(startup_32_vector_bsp - bsp_base)
+
+	.code32
+	.balign 4
+startup_32_bsp:
+
+	/* MKLINUX -- at this point, we're in 32-bit protected mode */
+
+	/* MKLINUX -- at this point, the segment selector registers point to
+	 * the beginning of the trampoline (usually 0x92000), and we need to 
+	 * change them to point to the beginning of the address space (0x0) */
+	cli
+        movl    $(__KERNEL_DS), %eax
+        movl    %eax, %ds
+        movl    %eax, %es
+        movl    %eax, %ss
+
+	/* MKLINUX -- from boot/compressed/head_64.S */
+
+	/* Load new GDT with the 64bit segments using 32bit descriptor.
+	 * The new GDT labels the entire address space as 64-bit, so we
+	 * can switch into long mode later. */
+        leal    (gdt_bsp_64 - bsp_base)(%esi), %eax
+        movl    %eax, (gdt_bsp_64 - bsp_base + 2)(%esi)
+        lgdt    (gdt_bsp_64 - bsp_base)(%esi)
+
+	/* Enable PAE mode.  Note that this does not actually take effect
+	 * until paging is enabled */
+	movl	%cr4, %eax
+        orl     $(X86_CR4_PAE), %eax
+        movl    %eax, %cr4
+
+	/* MKLINUX -- this is code from arch/x86/boot/compressed/head_64.S
+         * It's necessary here in order to set up pagetables to identity-map
+	 * the first 4 GB of the address space prior to entering the kernel. */
+
+        /* Initialize Page tables to 0 */
+	leal    (pgtable_bsp - bsp_base)(%esi), %edi
+	xorl    %eax, %eax
+        movl    $((4096*6)/4), %ecx
+        rep     stosl
+
+        /* Build Level 4 */
+        leal    (pgtable_bsp - bsp_base)(%esi), %edi
+        leal    0x1007 (%edi), %eax
+        movl    %eax, 0(%edi)
+
+        /* Build Level 3 */
+        leal    (pgtable_bsp - bsp_base + 0x1000)(%esi), %edi
+        leal    0x1007(%edi), %eax
+        movl    $4, %ecx
+1:      movl    %eax, 0x00(%edi)
+        addl    $0x00001000, %eax
+        addl    $8, %edi
+        decl    %ecx
+        jnz     1b
+
+        /* Build Level 2 */
+        leal    (pgtable_bsp - bsp_base + 0x2000)(%esi), %edi
+        movl    $0x00000183, %eax
+        movl    $2048, %ecx
+1:      movl    %eax, 0(%edi)
+        addl    $0x00200000, %eax
+        addl    $8, %edi
+        decl    %ecx
+        jnz     1b
+
+        /* Enable the boot page tables */
+        leal    (pgtable_bsp - bsp_base)(%esi), %eax
+        movl    %eax, %cr3
+
+        /* Enable Long mode in EFER (Extended Feature Enable Register) */
+        movl    $MSR_EFER, %ecx
+        rdmsr
+        btsl    $_EFER_LME, %eax
+        wrmsr
+
+        /*
+         * Setup for the jump to 64bit mode
+         *
+         * When the jump is performend we will be in long mode but
+         * in 32bit compatibility mode with EFER.LME = 1, CS.L = 0, CS.D = 1
+         * (and in turn EFER.LMA = 1).  To jump into 64bit mode we use
+         * the new gdt/idt that has __KERNEL_CS with CS.L = 1.
+         * We place all of the values on our mini stack so lret can
+         * used to perform that far jump.
+         */
+        pushl   $__KERNEL_CS
+        leal    (startup_64_bsp - bsp_base)(%esi), %eax
+        pushl   %eax
+
+	/* Enter paged protected Mode, activating Long Mode */
+        movl    $(X86_CR0_PG | X86_CR0_PE), %eax /* Enable Paging and Protected mode */
+        movl    %eax, %cr0
+
+	/* Jump from 32bit compatibility mode into 64bit mode. */
+        lret
+
+	.code64
+	.balign 4
+startup_64_bsp:
+
+	/* MKLINUX -- We should be in full 64-bit mode here, so we're 
+	 * able to jump to a kernel anywhere in the address space */
+
+	/* Get physical address of boot_params structure */
+	movq    (boot_params_phys_addr - bsp_base)(%rsi), %r15
+
+	/* Load kernel address into register */
+	movq    (kernel_phys_addr - bsp_base)(%rsi), %r14
+
+	/* Check whether the kernel is in the 4 GB we mapped already,
+	 * and if not, add an additional mapping */
+	movq	$0xffffffff00000000, %r8
+	testq	%r8, %r14
+	je	2f
+
+	/* If we got here, we need to identity-map an additional 1 GB */
+	
+	/* Mask off to figure out what our directory pointer should be */
+	movq	%r14, %r13
+	movq	$0xffffffffc0000000, %r12
+	andq	%r12, %r13
+
+	/* Set our PDPTE */
+	movq	%r13, %r11
+	shrq	$(30-3), %r11
+	leaq    (pgtable_bsp - bsp_base + 0x1000)(%rsi), %rdi
+	addq	%r11, %rdi
+	leaq	(pgtable_extra_bsp - bsp_base + 0x7)(%rsi), %rax
+	movq	%rax, 0(%rdi)
+
+	/* Populate the page directory */
+	leaq    (pgtable_extra_bsp - bsp_base)(%rsi), %rdi
+	movq    $0x00000183, %rax
+	addq	%r13, %rax
+	movq    $512, %rcx
+1:      movq    %rax, 0(%rdi)
+	addq    $0x00200000, %rax
+	addq    $8, %rdi
+	decq    %rcx
+	jnz     1b
+
+	/* Set esi to point to the boot_params structure */
+2:	movq	%r15, %rsi
+	jmp	*%r14
+
+	.align 8
+	ENTRY(boot_params_phys_addr)
+	.quad  0
+
+	.align 8
+	ENTRY(kernel_phys_addr)
+	.quad  0
+
+	.code16
+	.balign 4
+	# Careful these need to be in the same 64K segment as the above;
+tidt_bsp:
+	.word	0			# idt limit = 0
+	.word	0, 0			# idt base = 0L
+
+	# Duplicate the global descriptor table
+	# so the kernel can live anywhere
+	.balign 4
+tgdt_bsp:
+	.short	tgdt_bsp_end - tgdt_bsp		# gdt limit
+	.long	tgdt_bsp - bsp_base
+	.short 0
+	.quad	0x00cf9b000000ffff	# __KERNEL32_CS
+	.quad	0x00af9b000000ffff	# __KERNEL_CS
+	.quad	0x00cf93000000ffff	# __KERNEL_DS
+tgdt_bsp_end:
+
+	.code64
+	.balign 4
+gdt_bsp_64:
+        .word   gdt_bsp_64_end - gdt_bsp_64
+        .long   gdt_bsp_64 - bsp_base
+        .word   0
+        .quad   0x0000000000000000      /* NULL descriptor */
+        .quad   0x00af9a000000ffff      /* __KERNEL_CS */
+        .quad   0x00cf92000000ffff      /* __KERNEL_DS */
+        .quad   0x0080890000000000      /* TS descriptor */
+        .quad   0x0000000000000000      /* TS continued */
+gdt_bsp_64_end:
+
+	.code16
+	.balign 4
+startup_32_vector_bsp:
+	.long	startup_32_bsp - bsp_base
+	.word	__KERNEL32_CS, 0
+
+	.balign 4
+startup_64_vector_bsp:
+	.long	startup_64_bsp - bsp_base
+	.word	__KERNEL_CS, 0
+
+	.balign 4
+ENTRY(trampoline_status_bsp)
+	.long	0
+
+	.balign 4
+ENTRY(trampoline_location)
+	.quad   0
+
+trampoline_stack_bsp:
+	.fill 512,8,0
+trampoline_stack_bsp_end:
+
+ENTRY(trampoline_bsp_end)
+
+/*
+ * Space for page tables (not in .bss so not zeroed)
+ */
+        .balign 4096
+pgtable_bsp:
+        .fill 6*4096, 1, 0
+pgtable_extra_bsp:
+	.fill 1*4096, 1, 0
+
diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S
index 0f703f1..abb97d1 100644
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@ -209,6 +209,14 @@ SECTIONS
 		x86_trampoline_end = .;
 	}
 
+#ifdef CONFIG_POPCORN
+	 .x86_trampoline_bsp : AT(ADDR(.x86_trampoline_bsp) - LOAD_OFFSET) {
+		x86_trampoline_bsp_start = .;
+		*(.x86_trampoline_bsp)
+		x86_trampoline_bsp_end = .;
+	}
+#endif
+
 	.x86_cpu_dev.init : AT(ADDR(.x86_cpu_dev.init) - LOAD_OFFSET) {
 		__x86_cpu_dev_start = .;
 		*(.x86_cpu_dev.init)
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 9f548cb..d03e97e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -72,6 +72,8 @@
 #include <asm/smpboot_hooks.h>
 #include <asm/i8259.h>
 
+extern unsigned long orig_boot_params;
+
 /* State of each CPU */
 DEFINE_PER_CPU(int, cpu_state) = { 0 };
 
@@ -248,7 +250,7 @@ notrace static void __cpuinit start_secondary(void *unused)
 	 * fragile that we want to limit the things done here to the
 	 * most necessary things.
 	 */
-	cpu_init();
+	cpu_init(0);
 	preempt_disable();
 	smp_callin();
 
@@ -321,7 +323,7 @@ void __cpuinit smp_store_cpu_info(int id)
 
 	*c = boot_cpu_data;
 	c->cpu_index = id;
-	if (id != 0)
+	if (id != first_cpu(cpu_present_map))
 		identify_secondary_cpu(c);
 }
 
@@ -484,7 +486,7 @@ wakeup_secondary_cpu_via_nmi(int logical_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
 	int maxlvt;
-
+	printk(KERN_ERR"%s WANTS LOGICAL %d\n", __func__, logical_apicid);
 	/* Target chip */
 	/* Boot on the stack */
 	/* Kick the second */
@@ -518,7 +520,7 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 {
 	unsigned long send_status, accept_status = 0;
 	int maxlvt, num_starts, j;
-
+	printk(KERN_ERR"%s WANTS PHYSICAL %d START ip %lu\n",__func__, phys_apicid, start_eip);
 	maxlvt = lapic_get_maxlvt();
 
 	/*
@@ -664,6 +666,109 @@ static void __cpuinit announce_cpu(int cpu, int apicid)
 			node, cpu, apicid);
 }
 
+
+
+
+/*
+ * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
+ * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
+ * Returns zero if CPU booted OK, else error code from
+ * ->wakeup_secondary_cpu.
+ */
+
+int __cpuinit mkbsp_boot_cpu(int apicid, int cpu, unsigned long kernel_start_address)
+{
+	unsigned long boot_error = 0;
+	unsigned long start_ip;
+	int timeout;
+
+	/* POPCORN -- set physical address where kernel has been copied.
+	   Note that this needs to be written to the location where the
+	   trampoline was copied, not to the location within the original
+	   kernel itself. */
+
+	unsigned long *kernel_virt_addr = TRAMPOLINE_SYM_BSP(&kernel_phys_addr);
+	unsigned long *boot_params_virt_addr = TRAMPOLINE_SYM_BSP(&boot_params_phys_addr);
+	
+	*kernel_virt_addr = kernel_start_address;
+	*boot_params_virt_addr = orig_boot_params;
+
+	/* start_ip had better be page-aligned! */
+	start_ip = trampoline_bsp_address();
+
+	/*
+	 * This grunge runs the startup process for
+	 * the targeted processor.
+	 */
+
+	printk("Popcorn boot: CPU %d: start_ip = %lx\n", cpu, start_ip);
+
+	if (get_uv_system_type() != UV_NON_UNIQUE_APIC) {
+
+		pr_debug("Setting warm reset code and vector.\n");
+
+		smpboot_setup_warm_reset_vector(start_ip);
+		/*
+		 * Be paranoid about clearing APIC errors.
+		*/
+		if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid])) {
+			apic_write(APIC_ESR, 0);
+			apic_read(APIC_ESR);
+		}
+	}
+
+	/*
+	 * Kick the secondary CPU. Use the method in the APIC driver
+	 * if it's defined - or use an INIT boot APIC message otherwise:
+	 */
+	if (apic->wakeup_secondary_cpu)
+		boot_error = apic->wakeup_secondary_cpu(apicid, start_ip);
+	else
+		boot_error = wakeup_secondary_cpu_via_init(apicid, start_ip);
+
+	if (!boot_error) {
+
+		/*
+		 * Wait 5s total for a response
+		 */
+		for (timeout = 0; timeout < 50000; timeout++) {
+			udelay(100);
+
+			if (*(volatile u32 *)TRAMPOLINE_SYM_BSP(trampoline_status_bsp)
+			    == 0xA5A5A5A5) {
+				/* trampoline started but...? */
+				pr_info("CPU%d: Trampoline has started.\n", cpu);
+				break;
+			} else {
+				/* trampoline code not run */
+				pr_err("CPU%d: Not responding.\n", cpu);
+				boot_error = 1;
+				if (apic->inquire_remote_apic)
+					apic->inquire_remote_apic(apicid);
+			}
+
+			/*
+                         * Allow other tasks to run while we wait for the
+                         * AP to come online. This also gives a chance
+                         * for the MTRR work(triggered by the AP coming online)
+                         * to be completed in the stop machine context.
+                         */
+                        schedule();
+		}
+	}
+
+	if (boot_error) {
+		pr_err("Popcorn: boot error!!!\n");
+	}
+
+	/* mark "stuck" area as not stuck */
+	*(volatile u32 *)TRAMPOLINE_SYM_BSP(trampoline_status_bsp) = 0;
+
+	return boot_error;
+}
+
+
+
 /*
  * NOTE - on most systems this is a PHYSICAL apic ID, but on multiquad
  * (ie clustered apic addressing mode), this is a LOGICAL apic ID.
@@ -918,7 +1023,7 @@ static int __init smp_sanity_check(unsigned max_cpus)
 	preempt_disable();
 
 #if !defined(CONFIG_X86_BIGSMP) && defined(CONFIG_X86_32)
-	if (def_to_bigsmp && nr_cpu_ids > 8) {
+	if (def_to_bigsmp && nr_cpu_idssmp_store_cpu_info > 8) {
 		unsigned int cpu;
 		unsigned nr;
 
@@ -1031,6 +1136,7 @@ static void __init smp_cpu_index_default(void)
 void __init native_smp_prepare_cpus(unsigned int max_cpus)
 {
 	unsigned int i;
+	unsigned int cpu = first_cpu(cpu_present_map);
 
 	preempt_disable();
 	smp_cpu_index_default();
@@ -1038,17 +1144,17 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	/*
 	 * Setup boot CPU information
 	 */
-	smp_store_cpu_info(0); /* Final full version of the data */
-	cpumask_copy(cpu_callin_mask, cpumask_of(0));
+	smp_store_cpu_info(cpu); /* Final full version of the data */
+	cpumask_copy(cpu_callin_mask, cpumask_of(cpu));
 	mb();
 
-	current_thread_info()->cpu = 0;  /* needed? */
+	current_thread_info()->cpu = cpu;  /* needed? */
 	for_each_possible_cpu(i) {
 		zalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&per_cpu(cpu_llc_shared_map, i), GFP_KERNEL);
 	}
-	set_cpu_sibling_map(0);
+	set_cpu_sibling_map(cpu);
 
 
 	if (smp_sanity_check(max_cpus) < 0) {
@@ -1090,8 +1196,8 @@ void __init native_smp_prepare_cpus(unsigned int max_cpus)
 	 * Set up local APIC timer on boot CPU.
 	 */
 
-	printk(KERN_INFO "CPU%d: ", 0);
-	print_cpu_info(&cpu_data(0));
+	printk(KERN_INFO "CPU%d: ", cpu);
+	print_cpu_info(&cpu_data(cpu));
 	x86_init.timers.setup_percpu_clockev();
 
 	if (is_uv_system())
@@ -1157,6 +1263,63 @@ static int __init _setup_possible_cpus(char *str)
 }
 early_param("possible_cpus", _setup_possible_cpus);
 
+static DECLARE_BITMAP(setup_present_bits, CONFIG_NR_CPUS) __read_mostly;
+const struct cpumask *const setup_present_mask = to_cpumask(setup_present_bits);
+
+static int __init _setup_present_mask(char *str)
+{
+	cpulist_parse(str, (struct cpumask *)setup_present_mask);
+	return 0;
+}
+early_param("present_mask", _setup_present_mask);
+
+__init void prefill_present_map(void)
+{
+	int present;
+	int first;
+	char buffer[96];
+	memset(buffer, 0, 96);
+
+	//check present with possible
+	present = cpumask_weight(setup_present_mask);
+	first = cpumask_first(setup_present_mask);
+	// print present mask
+	cpumask_scnprintf(buffer, 96, setup_present_mask);
+	printk(KERN_INFO "%s: present_cpus %d %s, first %d, max_cpus %d\n",
+			__func__, present, buffer, first, setup_max_cpus);
+
+	//we assume that present was never settedif it is 0, prefill it with setup_max_cpus (if setted)
+	if (!present) {
+		int i;
+		for (i=0; i<setup_max_cpus; i++)
+			cpumask_set_cpu(i, (struct cpumask *) setup_present_mask);
+		present = cpumask_weight(setup_present_mask);
+		goto _finalize;
+	}
+
+	// present and setup_max_cpus must be synchronized, setup_max_cpus is imposed by the user
+	// does it affects hot plug? hopefully no
+	if (present != setup_max_cpus)
+		setup_max_cpus = present;
+
+	//figure out which is the current processor and change the present subset accordingly
+	cpumask_copy((struct cpumask *)cpu_present_mask, (struct cpumask *)setup_present_mask);
+
+	//adjust online and active cpu masks
+	cpumask_clear((struct cpumask *)cpu_online_mask);
+	cpumask_set_cpu(first_cpu(cpu_present_map), (struct cpumask *)cpu_online_mask);
+	cpumask_copy((struct cpumask *)cpu_active_mask, (struct cpumask *)cpu_online_mask);
+
+_finalize:
+	// TODO basically a check about the previous setting of cpu_online_mask and cpu_active_mask must be done
+	cpumask_scnprintf(buffer, 96, cpu_present_mask);
+        printk(KERN_INFO "%s: cpu_present_mask %s\n",__func__, buffer);
+        cpumask_scnprintf(buffer, 96, cpu_online_mask);
+        printk(KERN_INFO "%s: cpu_online_mask %s\n",__func__, buffer);
+        cpumask_scnprintf(buffer, 96, cpu_active_mask);
+        printk(KERN_INFO "%s: cpu_active_mask %s\n ",__func__, buffer);
+	return;
+}
 
 /*
  * cpu_possible_mask should be static, it cannot change as cpu's